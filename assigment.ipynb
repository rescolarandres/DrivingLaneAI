{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "# <font color=#003091> Assignment: Navigating traffic </font>\n",
    "### <font color=#0098df> Master in Computer Science & Business Technology</font>\n",
    "#### <font color=#a6a6a6> Year: 2023</font>  \n",
    "**David Kremer**\n",
    "\n",
    "\n",
    "This notebook provides a guideline for the Assignment, and shows the steps to build a RL algorithm that navigates through traffic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The RL Toolbox\n",
    "\n",
    "In the previous sessions we saw how to interact with environments through the Gym interface, how to collect data with the Agent class given a policy, and how to train a policy with the QPolicy class. \n",
    "\n",
    "The Memory, Agent and QPolicy classes can be loaded by simply importing them as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agent import Memory, Agent\n",
    "from rl.dqn import QPolicy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can be used in the same way as in the previous sessions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The environment\n",
    "\n",
    "For this assignment the environment is a traffic simulator that we can import in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from rl.traffic import TrafficEnv\n",
    "from rl.traffic import Car"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an environment variable that we can interact with in the same way as with the Gym environments, we just have to use the class TrafficEnv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrafficEnv(nlanes=4, ncars=5, images=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TrafficEnv accepts 3 arguments for creating the environment:\n",
    " - nlanes: number of lanes in the road\n",
    " - ncars: number of cars in the road\n",
    " - images(true/false): if true, the observations are images of the road, if false the observations are the state of environment\n",
    " \n",
    "![traffic_env3.png](attachment:traffic_env3.png)\n",
    " \n",
    "In the environment, the agent can control a car that is able to accelerate, break and change lanes to navigate through traffic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Task\n",
    "\n",
    "The task is to build and train a RL algorithm, using the provided toolbox, that interacts with the Traffic environment and achives the following goals:\n",
    " - Not crashing\n",
    " - Covering the most distance possible in a given time\n",
    " - Driving \"efficiently\" (that is, change lanes the least possible)\n",
    "\n",
    "The following sections outline the different steps to take to achive that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Understanding the Env\n",
    "\n",
    "Before starting the training and sampling, it is important to understand the environment. In particular, it is important to understand the action space and the observation space.\n",
    "\n",
    "In the case of our particular environment, we have two choices of observations: images from the road, or the road's state (that is, positions and velocities of the cars). \n",
    "\n",
    "In this stage you should:\n",
    " - Understand and describe the shape and type of the action space\n",
    " - Understand and describe the shape and type of the observation space for both the images and the state cases.\n",
    " - Decide which of the two observation spaces you will choose, and justify your decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space shape: (25, 50)\n",
      "Action space discrete shape: Discrete(3)\n",
      "Action space box shape: Box(0.0, 1.0, (2,), float64)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of observation space and action space\n",
    "print(\"Observation space shape:\", env.observation_space.shape)\n",
    "\n",
    "print(\"Action space discrete shape:\", env.action_space[0])\n",
    "print(\"Action space box shape:\", env.action_space[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space is divided into a tuple composed of 2 elements:\n",
    "- The first element corresponds to three integer values: 0 for moving left, 1 for straight and 2 to the right\n",
    "- The second value corresponds to a list of two values, the first is the acceleration, and the second braking.\n",
    "\n",
    "For the observation space:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Sampling from the Env\n",
    "\n",
    "Once the action and observation spaces are known, we should do some \"manual\" sampling to familiarize ourselves with the environment.\n",
    "\n",
    "In this stage you should:\n",
    " - Rendering the environment and manually input actions.\n",
    " - Use the previous functionality to understand what each of the actions mean (and describe them).\n",
    " - Use the previous functionality to understand what the final states are (when the simulator returns a end_state). \n",
    " - Build a function (as in previous sessions) that samples the environment with the random policy, and collects the [observation, action, reward, observation, final_state] data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the environment and render the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.  , 0.75, 0.  , ..., 0.  , 0.75, 0.  ],\n",
       "        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "        ...,\n",
       "        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "        [0.  , 0.75, 0.  , ..., 0.  , 0.75, 0.  ]]),\n",
       " {})"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "#env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating some inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = env.action_space.sample()\n",
    "# for i in range(50):\n",
    "#     env.step(action)\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, array([0.32556457, 0.89107113]))"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(20):\n",
    "#     env.step((2,[1,0]))\n",
    "#     env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.is_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3125    ,  0.01604039,  0.        , -0.25      ,  0.05308255,\n",
       "        0.0625    ,  0.00132598,  0.        ,  0.45590264,  0.3125    ,\n",
       "        0.        , -0.125     , -0.47799641,  0.1875    , -0.00082204,\n",
       "       -0.125     ,  0.3083678 ,  0.1875    , -0.00082204])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_state()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The states give an array composed by:\n",
    "- Three elements first elements corresponding to the agend: position, velocity and acceleration of the agent.\n",
    "- Four elements for each other car: relative x-y position wrt the agent, the absolute x position and velocity.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function that assigns a random policy and ouptuts observation, action, reward, observation and final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSample(env):\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)\n",
    "    obs = env.get_state()\n",
    "    changing = True if obs[2] != 0 else False\n",
    "    final_state = env.is_final\n",
    "    reward = env.reward_func(obs[0], obs[1], changing, final_state)\n",
    "    return [obs, action, reward, final_state]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Defining the reward function\n",
    "\n",
    "As you may have noticed from the previous section, the reward is always 0. That is because the environment does not define a reward function.\n",
    "\n",
    "In this stage you should:\n",
    " - Define a reward function that will make the agent achive the goals described above (and justify your choice).\n",
    " - Test the reward function with the random policy, and obtain the average reward over 10 episodes. \n",
    " \n",
    "The reward function is a function that, at each frame, returns the reward just obtained at that frame. For building the reward function you have access to:\n",
    " - x_agent(float): the 'x' position of the agent (0 to 1, from left to right)\n",
    " - v_agent(float): the velocity of the agent\n",
    " - changing(bool): true if agent is changing lane, false otherwise\n",
    " - crashed(bool): true if agent just crashed in that frame, false otherwise\n",
    " \n",
    "The following code shows how to define the function and include it in the env:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(x_agent, v_agent, changing, crashed):\n",
    "    reward = 0\n",
    "    if crashed:\n",
    "        reward -= 100\n",
    "    else:\n",
    "        reward += 10\n",
    "        if changing:\n",
    "            reward -= 1\n",
    "        if v_agent > 0.75:  # if velocity is close to maximum, provide high reward.\n",
    "            reward += 10\n",
    "        elif v_agent > 0.5:  # if velocity is above average, provide moderate reward.\n",
    "            reward += 5\n",
    "        else:  # if velocity is below average, penalize.\n",
    "            reward -= 5\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_func = reward_func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clue*: you don't _need_ to use all the input variables to compute the reward, some of them may be not useful for the task..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Preparing the Agent\n",
    "\n",
    "The action space contains a continuous component. If we want to be able to use the DQN algorithm to learn a policy we will have to discretize it. \n",
    "\n",
    "In this stage you should:\n",
    " - Create a wrapper that transforms current environment into one with a completely discrete action space.\n",
    " - If you chose to use images, you may need to also wrap the environment with the Stack wrapper (an observation wrapper, provided below) that stacks frames into 3 (as we did with FlappyBird).\n",
    " - Create an Agent and test it with the random policy for this new discrete environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "# import numpy as np\n",
    "\n",
    "# class Stack(gym.ObservationWrapper):\n",
    "#     def __init__(self, ienv, frames=3):\n",
    "#         super(Stack, self).__init__(ienv)   \n",
    "#         self.frames = frames\n",
    "#         self.im_size = tuple(ienv.observation_space.shape)\n",
    "#         self.c_obs = np.zeros((self.frames,) + self.im_size)\n",
    "#         self.observation_space = gym.spaces.Box(low=0, high=1, shape=self.c_obs.shape)\n",
    "\n",
    "#     def observation(self, obs):\n",
    "#         self.c_obs = np.concatenate( [self.c_obs[1:,:], obs[np.newaxis,:]], axis=0 )\n",
    "#         return self.c_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiles in ./.venv/lib/python3.10/site-packages (1.0.4)\n",
      "Requirement already satisfied: future in ./.venv/lib/python3.10/site-packages (from tiles) (0.18.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from tiles) (3.1)\n"
     ]
    }
   ],
   "source": [
    "# # example:\n",
    "# stacked_env = Stack(env, frames=4)\n",
    "!pip install tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class Stack(TrafficEnv):\n",
    "    def __init__(self, nlanes, ncars):\n",
    "        super().__init__(nlanes, ncars)\n",
    "        self.l_lims = np.array((0.0, 0.0)) # f, b\n",
    "        self.h_lims = np.array((2.0, 1.0)) # f, b\n",
    "        self.action_space = spaces.Tuple((spaces.Discrete(3),spaces.Box(self.l_lims, self.h_lims, dtype=int)))\n",
    "    \n",
    "\n",
    "\n",
    "# Define the discrete actions\n",
    "discrete_actions = [(0, [0.0, 0.0]), (0, [0.0, 0.5]), (0, [0.0, 1.0]),  # Left with different acceleration/braking levels\n",
    "                    (0, [0.5, 0.0]), (0, [0.5, 0.5]), (0, [0.5, 1.0]),  # Left with different acceleration/braking levels\n",
    "                    (0, [1.0, 0.0]), (0, [1.0, 0.5]), (0, [1.0, 1.0]),  # Left with different acceleration/braking levels\n",
    "\n",
    "                    (1, [0.0, 0.0]), (1, [0.0, 0.5]), (1, [0.0, 1.0]),  # Straight with different acceleration/braking levels\n",
    "                    (1, [0.5, 0.0]), (1, [0.5, 0.5]), (1, [0.5, 1.0]),  # Straight with different acceleration/braking levels\n",
    "                    (1, [1.0, 0.0]), (1, [1.0, 0.5]), (1, [1.0, 1.0]),  # Straight with different acceleration/braking levels\n",
    "\n",
    "                    (2, [0.0, 0.0]), (2, [0.0, 0.5]), (2, [0.0, 1.0]),  # Right with different acceleration/braking levels\n",
    "                    (2, [0.5, 0.0]), (2, [0.5, 0.5]), (2, [0.5, 1.0]),  # Right with different acceleration/braking levels\n",
    "                    (2, [1.0, 0.0]), (2, [1.0, 0.5]), (2, [1.0, 1.0])]  # Right with different acceleration/braking levels\n",
    "\n",
    "\n",
    "# Map continuous actions to discrete actions\n",
    "def map_action(action):\n",
    "    return discrete_actions[action]\n",
    "\n",
    "# Wrap the environment with a custom wrapper to handle the mapping of actions\n",
    "class DiscreteActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.action_space = spaces.Discrete(len(discrete_actions))\n",
    "    \n",
    "    def action(self, action):\n",
    "        return map_action(action)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Stack(4,5) # Add this line after initializing your environment\n",
    "# env.reward_func = reward_func\n",
    "# env.reset()\n",
    "# #env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample = randomSample(env)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clue*: You may want to define the actions \"manually\", that is, define the act_dict dictionary by hand specifying a list of discrete actions that are combinations of the actions of the original space. In that way you can also filter combinations that don not make sense, like breaking and accelerating at the same time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clue2*: If you chose to use images, you will have to apply two wrappers to the env. You can do this by simply wrapping an alredy wrapped env, and in this case the order should not matter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Preparing the DQN\n",
    "\n",
    "With the agent ready, we just need to prepare the DQN algorithm. For that, we need to define the neural network for the Q function.\n",
    "\n",
    "In this stage you should:\n",
    "- Define the NN architecture. Remember the input shape will be the observation space, and the output shape should be the action space. (In particular, if you are using images, you may want to do a CNN).\n",
    "- Implement the NN in Pytorch and create a QPolicy with that NN.\n",
    "- Test it by collecting data with the agent created in the previous section, this time using the policy given by the untrained QPolicy instead of a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn import MlpPolicy\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# Saving logs to visulise in Tensorboard, saving models\n",
    "models_dir = f\"models/Highway-{time.time()}\"\n",
    "logdir = f\"logs/Highway-{time.time()}\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "\n",
    "\n",
    "# # The learning agent and hyperparameters\n",
    "# model = DQN(\n",
    "#     policy=MlpPolicy,\n",
    "#     env=env,\n",
    "#     verbose=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/austinbrees/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/Users/austinbrees/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/.venv/lib/python3.10/site-packages/gym/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.76e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 402      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 7030     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 404      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 12390    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.36e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 405      |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 16283    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.23e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 405      |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 19721    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.13e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 408      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 22618    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.02e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 407      |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 24375    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 981      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 407      |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 27478    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 909      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 407      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 29082    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 897      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 407      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 32305    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 867      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 406      |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 34666    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 870      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 406      |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 38262    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[456], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m TIMESTEPS \u001b[39m=\u001b[39m \u001b[39m20000\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m     model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mTIMESTEPS, reset_num_timesteps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, tb_log_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDQN\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     63\u001b[0m     model\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodels_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mTIMESTEPS\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m(i\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[39m# Save the trained model\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/.venv/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    259\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[1;32m    260\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    268\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    269\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    270\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    271\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    272\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    273\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    274\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/.venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:312\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    311\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 312\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    313\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    314\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    315\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    316\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    317\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    318\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    319\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    320\u001b[0m     )\n\u001b[1;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/.venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:544\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    541\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[1;32m    543\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[1;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    547\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/.venv/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/.venv/lib/python3.10/site-packages/shimmy/openai_gym_compatibility.py:123\u001b[0m, in \u001b[0;36mGymV26CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[ObsType, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgym_env\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/.venv/lib/python3.10/site-packages/gym/core.py:460\u001b[0m, in \u001b[0;36mActionWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    459\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Runs the environment :meth:`env.step` using the modified ``action`` from :meth:`self.action`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction(action))\n",
      "File \u001b[0;32m~/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/rl/traffic.py:95\u001b[0m, in \u001b[0;36mTrafficEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m     car0y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcars[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpy\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcars[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m---> 95\u001b[0m         c\u001b[39m.\u001b[39;49mstep(\u001b[39m0.0\u001b[39;49m, \u001b[39m1.0\u001b[39;49m)\n\u001b[1;32m     96\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcars[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstep(action[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m0.0005\u001b[39m, (\u001b[39m1.0\u001b[39m\u001b[39m-\u001b[39maction[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m])\u001b[39m*\u001b[39m\u001b[39m0.0001\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.99989\u001b[39m)\n\u001b[1;32m     98\u001b[0m car0y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcars[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpy\n",
      "File \u001b[0;32m~/Documents/MCSBT/Term-3/deep-learning/driving-ai/DrivingLaneAI/rl/traffic.py:178\u001b[0m, in \u001b[0;36mCar.step\u001b[0;34m(self, f, b)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mfor\u001b[39;00m ln, lx \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanes):\n\u001b[1;32m    176\u001b[0m     reached_lane \u001b[39m=\u001b[39m (((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpx \u001b[39m-\u001b[39m lx) \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m) \u001b[39mand\u001b[39;00m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpx \u001b[39m-\u001b[39m lx) \u001b[39m>\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.005\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mva \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    177\u001b[0m                    (((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpx \u001b[39m-\u001b[39m lx) \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m) \u001b[39mand\u001b[39;00m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpx \u001b[39m-\u001b[39m lx) \u001b[39m<\u001b[39m  \u001b[39m0.005\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mva \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m))\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mif\u001b[39;00m reached_lane:\n\u001b[1;32m    179\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mva \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpx \u001b[39m=\u001b[39m lx\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import torch\n",
    "\n",
    "# Create the environment\n",
    "env = TrafficEnv(nlanes=4, ncars=5)\n",
    "env.reward_func = reward_func\n",
    "stacked_env = Stack(nlanes=4, ncars=5)\n",
    "stacked_env.reset()\n",
    "\n",
    "# Wrap the environment with the DiscreteActionWrapper to handle the mapping of actions\n",
    "env = DiscreteActionWrapper(stacked_env)\n",
    "\n",
    "\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# Define hyperparameters\n",
    "policy = \"MlpPolicy\"  # Policy architecture\n",
    "learning_rate = 0.001  # Learning rate\n",
    "buffer_size = 10000  # Replay buffer size\n",
    "batch_size = 32  # Minibatch size\n",
    "gamma = 0.99  # Discount factor\n",
    "exploration_fraction = 0.1  # Fraction of time spent exploring\n",
    "exploration_initial_eps = 1.0  # Initial exploration rate\n",
    "exploration_final_eps = 0.02  # Final exploration rate\n",
    "target_update_interval = 1000  # Update the target network every X steps\n",
    "train_freq = 1  # Update the model every X steps\n",
    "gradient_steps = 1  # Number of gradient steps to take during training\n",
    "policy_kwargs = dict()  # Additional arguments for the policy\n",
    "\n",
    "# Initialize the DQN model with hyperparameters\n",
    "model = DQN(\n",
    "    policy=policy,\n",
    "    env=env,\n",
    "    learning_rate=learning_rate,\n",
    "    buffer_size=buffer_size,\n",
    "    batch_size=batch_size,\n",
    "    gamma=gamma,\n",
    "    exploration_fraction=exploration_fraction,\n",
    "    exploration_initial_eps=exploration_initial_eps,\n",
    "    exploration_final_eps=exploration_final_eps,\n",
    "    target_update_interval=target_update_interval,\n",
    "    train_freq=train_freq,\n",
    "    gradient_steps=gradient_steps,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "TIMESTEPS = 20000\n",
    "\n",
    "for i in range(10):\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"DQN\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS * (i + 1)}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"dqn_traffic\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward}\")\n",
    "\n",
    "# To load the trained model\n",
    "model = DQN.load(\"dqn_traffic\")\n",
    "\n",
    "# Test the trained agent\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    print(\"Observation:\", obs)\n",
    "    action, _states = model.predict(obs)\n",
    "    print(\"Predicted action:\", action)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    print(\"Action executed:\", action)\n",
    "    env.render()\n",
    "    if dones:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Training the DQN\n",
    "\n",
    "With all the \"ingredients\" ready, we can finally train the policy.\n",
    "\n",
    "In this stage you should:\n",
    " - Define and implement the training loop (as we did in previous sessions).\n",
    " - Train the policy and collect training stats such as mean reward per iteration.\n",
    " - Display the training stats and mean rewards obtained by your agent.\n",
    " - (optional) Tune the hyperparameters to get better results. \n",
    " - (optional) Visualize the Q values during an episode.\n",
    " - (optional) Enjoy watching how your agent drives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clue*: Training the agent may take a long time and it may be difficult to find good hyperparameters, so if it does not converge quickly it does not necessarily mean you did something wrong. If you want to try, a road with fewer cars (or just 1 car) and fewer lanes (like 3) will be easier to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `TrafficEnv` and `DiscreteActionWrapper` are defined somewhere\n",
    "env = TrafficEnv(nlanes=4, ncars=5)\n",
    "env = DiscreteActionWrapper(env)\n",
    "\n",
    "# Load the model\n",
    "models_dir = \"models/Highway-1688288612.0843751\"\n",
    "model_path = f\"{models_dir}/160000\"\n",
    "best_model = DQN.load(model_path, env=env)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "while True:\n",
    "    action, _states = best_model.predict(obs)\n",
    "    action = int(action)\n",
    "    step_output = env.step(action)  # Call step and store result in variable\n",
    "\n",
    "    # Ensure you're handling the correct number of returned values\n",
    "    if len(step_output) == 5:\n",
    "        obs, rewards, dones, extra_value, info = step_output\n",
    "    else:\n",
    "        obs, rewards, dones, info = step_output\n",
    "\n",
    "    \n",
    "    \n",
    "    env.render()\n",
    "\n",
    "    # Convert numpy array to PIL Image\n",
    "    \n",
    "    if dones:\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
